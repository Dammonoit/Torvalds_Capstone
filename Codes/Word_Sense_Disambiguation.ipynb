{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's import all the libraries i.e nltk specially which is used for NLP\n",
    "import nltk\n",
    "import codecs\n",
    "from googletrans import Translator\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in this function of simple filter we'll pass the sentences to filter them by removing the unesscary words i.e \n",
    "#'the' 'is' etc.\n",
    "def simpleFilter(sentence):\n",
    "    #this list will store only useful words.\n",
    "    filtered_sent = []   \n",
    "    #lemmatizer outputs the correct word according to dictionary for eg: cacti would be cactus, rocks would be rock\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #the stop words contains the list of all the stop words present in \"english\" or any mentioned language.\n",
    "    #stopwords like i,me,the,they,would,wouln't etc\n",
    "    #stop words also has additional keywords pos for adjectives,verbs but noun is defualt.\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    #word tokenizer tokenizes(breaks downs) the sentence in individual words.\n",
    "    words = word_tokenize(sentence)\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sent.append(lemmatizer.lemmatize(w))\n",
    "\n",
    "    return filtered_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#similaritycheck function checks the similarity between the tokens(useful words) returned from the first function\n",
    "#and the training dataset i.e both .txt files \n",
    "#for this synonyms is checked for each word and the word in the .txt file and output is returned on a scale of 0-1\n",
    "#this is main function for ambiguity check and then normalized similarity between sentences is stored.\n",
    "def simlilarityCheck(word1, word2):\n",
    "\n",
    "    word1 = word1 + \".n.01\"\n",
    "    word2 = word2 + \".n.01\"\n",
    "    try:\n",
    "        w1 = wordnet.synset(word1)\n",
    "        w2 = wordnet.synset(word2)\n",
    "\n",
    "        return w1.wup_similarity(w2)\n",
    "\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function is used to create synonyms from the word passed as a argument.\n",
    "def synonymsCreator(word):\n",
    "    synonyms=[]\n",
    "    #wordnet.synsets creates all possible versions of the word passed in order to create synonyms.\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for i in syn.lemmas():\n",
    "            #syn.lemmas() creates the object inorder to create the relationship between words created by \n",
    "            #wordnet.synsets \n",
    "            synonyms.append(i.name())\n",
    "\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we apply the next level filter, function: filteredSentence , to apply lemmatization over stemmed tokens and again removing stop words. \n",
    "#In the filtered sentence list, we now store the token word along with its synonyms for more precised matching / similarity check. \n",
    "def filteredSentence(sentence):\n",
    "\n",
    "    filtered_sent = []\n",
    "    lemmatizer = WordNetLemmatizer()  # lemmatizes the words\n",
    "    ps = PorterStemmer()  # stemmer stems the root of the word.\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(sentence)\n",
    "\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sent.append(lemmatizer.lemmatize(ps.stem(w)))\n",
    "            for i in synonymsCreator(w):\n",
    "                filtered_sent.append(i)\n",
    "    return filtered_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query: this shoe is awesome\n",
      "Positive Sentiment\n",
      "\n",
      "TERMINATED\n",
      "Enter Query: end\n",
      "Positive Sentiment\n",
      "\n",
      "TERMINATED\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "     \n",
    "    #this is the first file to which contains info about negative sentiments\n",
    "    negative_sentiments = codecs.open(\"/home/dharmendra/Desktop/desk/captsone/senti_outputs/neg_senti.txt\", 'r', \"utf-8\") \n",
    "    sent2 = negative_sentiments.read().lower()   \n",
    "    #this is the second file which contains info about positive sentiments\n",
    "    positive_sentiments = codecs.open(\"/home/dharmendra/Desktop/desk/captsone/senti_outputs/pos_senti.txt\", 'r', 'utf-8')\n",
    "    sent1 = positive_sentiments.read().lower()\n",
    "    sent3 = \"start\"\n",
    "    \n",
    "    while(sent3 != \"end\"):\n",
    "        #let's take input of the new sentence over here and it'll be automatically converted into lowercase.\n",
    "        sent3 = input(\"Enter Query: \").lower()\n",
    "        #here the object from google cloud api which transaltes the any language into english\n",
    "        translator = Translator()\n",
    "        sent3=translator.translate(sent3).text\n",
    "        \n",
    "        filtered_sent1 = []\n",
    "        filtered_sent2 = []\n",
    "        filtered_sent3 = []\n",
    "\n",
    "        counter1 = 0\n",
    "        counter2 = 0\n",
    "        sent31_similarity = 0\n",
    "        sent32_similarity = 0\n",
    "\n",
    "        #these all three functions will store the filtered sentence from function above.\n",
    "        #sentence 1 contains txt about cricket bat\n",
    "        #sentence 2 contains txt about vampire bat\n",
    "        #sentence 3 is our input query.\n",
    "        filtered_sent1 = simpleFilter(sent1)\n",
    "        filtered_sent2 = simpleFilter(sent2)\n",
    "        filtered_sent3 = simpleFilter(sent3)\n",
    "\n",
    "        #in the following function we'll pass each and every word of the both the sentences with the\n",
    "        #each and every word of the input query.\n",
    "        for i in filtered_sent3:\n",
    "\n",
    "            for j in filtered_sent1:\n",
    "                counter1 = counter1 + 1\n",
    "                sent31_similarity = sent31_similarity + simlilarityCheck(i, j)\n",
    "\n",
    "            for j in filtered_sent2:\n",
    "                counter2 = counter2 + 1\n",
    "                sent32_similarity = sent32_similarity + simlilarityCheck(i, j)\n",
    "\n",
    "        filtered_sent1 = []\n",
    "        filtered_sent2 = []\n",
    "        filtered_sent3 = []\n",
    "\n",
    "        filtered_sent1 = filteredSentence(sent1)\n",
    "        filtered_sent2 = filteredSentence(sent2)\n",
    "        filtered_sent3 = filteredSentence(sent3)\n",
    "\n",
    "        sent1_count = 0\n",
    "        sent2_count = 0\n",
    "\n",
    "        for i in filtered_sent3:\n",
    "\n",
    "            for j in filtered_sent1:\n",
    "\n",
    "                if(i == j):\n",
    "                    sent1_count = sent1_count + 1\n",
    "\n",
    "            for j in filtered_sent2:\n",
    "                if(i == j):\n",
    "                    sent2_count = sent2_count + 1\n",
    "\n",
    "        if((sent1_count + sent31_similarity) > (sent2_count+sent32_similarity)):\n",
    "            print(\"Positive Sentiment\")\n",
    "            #now let's append it to the respective files.\n",
    "            with open(\"/home/dharmendra/Desktop/desk/captsone/senti_outputs/pos_senti.txt\", \"a\") as myfile:\n",
    "                myfile.write(sent3)\n",
    "        else:\n",
    "            print(\"Negative Sentiment\")\n",
    "            with open(\"/home/dharmendra/Desktop/desk/captsone/senti_outputs/neg_senti.txt\", \"a\") as myfile:\n",
    "                myfile.write(sent3)\n",
    "\n",
    "        print(\"\\nTERMINATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
